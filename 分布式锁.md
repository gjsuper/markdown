#分布式锁

[TOC]

## 一.分布式锁应用场景

在单机环境下，处理并发请求和保护共享变量是比较容易的，利用concurrent包中的并发工具，可以很好地处理单个jvm中的多线程并发问题。

但是，当扩展到分布式系统中时，问题就不一样了，现有的单机并发控制策略在分布式的环境下失效了，java本身提供的互斥锁并不能解决多个jvm间的并发问题，因此需要一种能应用在分布式集群中、跨jvm的互斥机制来保护共享资源。这就是分布式锁要做的事。

## 二.分布式锁应具备的条件

* 互斥性：同一时间只能有一个请求获得锁

* 高性能：分布式往往意味着请求并发量大，因此分布式锁应具备很好的性能

* 可重入：同一个线程可多次获得同一把锁

* 防死锁：一个机器请求到分布式锁后，还没来得及释放锁就宕机了，如果没有防过期死锁的机制，其他机器就永远获取不到锁了

## 三.分布式锁实现

实现分布式锁的方式有很多种，例如基于数据库、zookeeper、分布式锁，另外美团内部也有一些分布式锁的实现方式。

目前在我们小组的项目中，几乎都是基于缓存来实现分布式锁的，尤其是redis，因此本文的重点会放在基于缓存的实现和分析上，最后会给出redis的作者对分布式锁的思考和美团基础平台的分布式锁实现。

### 3.1 基于数据库实现

基于数据库实现分布式锁有很多种方式：例如乐观锁、悲观锁、唯一键约束

#### 3.1.1 基于乐观锁实现分布式锁

乐观锁假设正常情况下不会发生锁冲突，只有在真正更新数据的时候才会去检查时候是否有数据冲突。

乐观锁的实现方式就是在数据表中加一个字段，来作为该行数据的版本标志，这个字段可以是一个时间戳或者是一个版本号。加锁的时候先去查询该行数据的版本并将版本保存起来，然后去修改该行数据，修改时需检查自己保存的版本和数据库里的版本是否一致，如果一致就更新版本号（将版本号+1或者是更新时间戳），并获得锁。如果版本号不一致，就表示获取锁失败。示例如下：

#### 3.1.2 基于悲观锁实现分布式锁

基于悲观锁实现分布式锁是最简单粗暴的方式，就是直接在数据库上加锁，但是这种方式需在字段是加唯一索引

public class DBdistributedLock {
    private Connection connection;
    private static final String cmd = "select * from lock where id = 1 for update";

		//加锁
    public void lock(CallBack callBack)  {
        try {
            connection.setAutoCommit(false);
            st = conn.prepareStatement(cmd);
            ResultSet rs = st.executeQuery();

            if(st == null) {
            	return false;
            }
            return true;
        } catch (SQLException e) {
            e.printStackTrace();
        } finally {
            connection.close()
        }
            return false;
    }

    //释放锁
    public void unlock() {
    	connection.commit();
    }
}

#### 3.1.3 基于唯一键实现分布式锁

这种方式就是在表字段上加唯一索引，这里在id字段上加唯一索引，然后：

//加锁
insert into locker(id) values (1);

//释放锁
delete from lcoker where id = 1;

优缺点

优点：好理解、实现简单

缺点：

锁没有过期时间，一旦某个jvm获取到锁后就宕机了，没有释放锁，其他jvm就再也获取不到锁了

行级锁靠不住。我们在字段上加了唯一索引，期望的是db能走索引，只锁住某一行。但经过db的优化，可能并不会走索引，而是走全表扫描，尤其是在表的数据量较小的情况下，这时就会形成表锁

高并发下性能不太理想

如果长时间不释放锁或者在并发量较大情况下，数据库的连接会越来越多，可能会把连接池撑爆，会影响到正常业务

### 3.2 基于zookeeper实现

#### 3.2.1 利用临时顺序节点实现

zookeeper里有一种节点叫临时顺序节点，想要获取锁的线程都去同一个目录下创建临顺序节点，节点序号最小的那个线程获得锁。否则监听目录下的子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁

羊群效应：假如当前有大量的节点在等待锁，如果获得锁的客户端释放锁时，那么所有等待锁的客户端都会被唤醒，这种情况称为“羊群效应”；

在这种羊群效应中，zookeeper需要通知所有等待锁的客户端，这会阻塞其他的操作，最好的情况应该只唤醒新的最小节点对应的客户端。因此最好的做法是，让等待锁的节点去监听序号比他小的最大节点，例如序号为99的节点去监听序号为98的节点。示例代码如下：

zookeeper实现分布式锁

#### 3.2.2 利用节点名称的唯一性实现

zookeeper的同一个目录下每个节点的名称都是唯一的，所以可以在获取锁的时候在目录下创建临时节点，只有创建成功的节点才能获取到锁，释放锁的时候就将该节点删除，获取锁失败的客户端可以选择监听该节点，得到删除节点的监听响应后再去争夺锁。

优缺点

优点：不依靠超时时间释放锁；可靠性高；系统要求高可靠性时，建议采用zookeeper锁。

缺点：会频繁地创建和删除节点，性能比不上缓存锁

利用zookeeper实现分布式锁本身不算难，但有一些问题还是需要关注，后面在论述Redis和Redlock时，会对基于zookeeper的分布式锁做更详细的分析

### 3.3 基于缓存实现

目前常用的缓存主要是cellar和redis，因此基于缓存的分布式锁也主要是利用他俩来实现的，这里主要对基于redis的分布式锁做分析。

现在团队内部主要使用redis实现，可以直接使用setnx接口来完成，如果加锁成功，返回true，否则返回false。因此下面主要对redis的分布式锁作分析。

#### 3.3.1 基于redis的实现

* 第一个问题：过期时间

    一个锁需要有一个过期时间来保证获取到锁的客户端在某些异常情况下能够释放掉锁。例如一个客户端获取锁之后就崩溃了，或者由于发生了网络分割导致它再也无法和Redis节点通信了，那么它就会一直持有这个锁，而其它客户端就永远无法获得锁了。一个好的锁需要有一种机制来防止这个问题发生，例如过期时间。同时获得锁的客户端必须在过期时间之内完成对共享资源的访问，否则共享资源可能会得不到保护。

* 第二个问题：唯一值

    获取锁的时候，应该给key设置一个唯一值，这个唯一值的作用是保证客户端释放掉的是自己的锁，否则可能会发生下面的问题：

    客户端1获取锁成功。

    客户端1在某个操作上阻塞了很长时间。

    过期时间到了，锁自动释放了。

    客户端2获取到了对应同一个资源的锁。

    客户端1从阻塞中恢复过来，释放掉了客户端2持有的锁。

    之后就没有锁为客户端2的共享资源提供保护了。



* 第三个问题：原子性

   获取锁的操作和释放锁的操作需要是原子性的。

   获取锁需要原子性：下面这种获取锁的写法就不太好，因为获取锁和设置过期时间不是原子操作：

```Java
public static void badTryLock (String lockKey, String requestId, int expireTime) {
    Long result = Redis.setnx(lockKey, requestId);
    //判断是否获取到锁，这里可能会有问题，如果在setnx之后就宕机了，下面的过期时间就设置不进去了
    if (result == 1) {
        Redis.expire(lockKey, expireTime);
    }
}
```

因此一种比较好的写法应该是这样的：

```Java
public static boolean bestTryLock(String lockKey, String requestId, int expireTime) {
				//可以同时设置值和过期时间，在redis内部，这是一个原子操作
        String result = Redis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime);
        if (LOCK_SUCCESS.equals(result)) {
            return true;
        }
        return false;
    }
```

释放锁需要原子性：获取锁的时候，我们给key设置了一个唯一值，那么在释放锁的时候，就需要判断一个我们删除的key对应的值是否是我们设置的值，如果是，就释放掉锁。如果不做这个判断的话，就可能会释放掉别人的锁。但是”获取值“+”判断“+”删除“这三个操作加起来应该是原子性的，否则会出现下面的问题：

1. 客户端1获取锁成功。

2. 客户端1访问共享资源。

3. 客户端1为了释放锁，先执行'GET'操作获取随机字符串的值。

4. 客户端1判断随机字符串的值，与预期的值相等。

5. 客户端1由于某个原因阻塞住了很长时间。

6. 过期时间到了，锁自动释放了。

7. 客户端2获取到了对应同一个资源的锁。

8. 客户端1从阻塞中恢复过来，执行DEL操纵，释放掉了客户端2持有的锁。

因此一种好的实现方式应该是下面这样的：

```Java
public class RedisLocker {
    private static final Long RELEASE_SUCCESS = 1L;

    public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) {
        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

        if (RELEASE_SUCCESS.equals(result)) {
            return true;
        }
        return false;
    }
}
```

首先获取锁对应的value值，检查是否与requestId相等，如果相等就释放锁。为了确保上述操作是原子性的，这里使用了Lua语言，在eval命令执行Lua代码的时候，Lua代码将被当成一个命令去执行，并且直到eval命令执行完成，Redis才会执行其他命令。

上面这三个问题，只要稍加注意，是可以避免的，但有一个问题是redis分布式锁不能解决的，就是单点问题。

4. 第四个问题：单点问题和Redlock

为了提高可用性，我们往往会部署一个redis集群，当Master节点不可用的时候，系统自动切到Slave上（failover）。但由于Redis的主从复制（replication）是异步的，这可能导致在failover过程中丧失锁的安全性。

这时基于redis集群的分布式锁就会有问题，如下:

客户端1从Master获取了锁（成功设置了key-value）

Master宕机了，存储锁的key还没有来得及同步到Slave上。

Slave升级为Master

客户端2从新的Master获取到了对应同一个资源的锁

针对该问题，redis的作者提出了一种解决方案，就是Redlock。在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。获取锁的基本流程：

获取当前Unix时间，以毫秒为单位。

依次尝试从5个实例，使用相同的key和具有唯一性的value（例如UUID）获取锁。当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试去另外一个Redis实例请求获取锁。

客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（N/2+1，这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。

如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。

如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功，为了防止某些节点获取到锁但是客户端没有得到服务端返回的响应）。

这种方式可以很大程度的提高锁的安全性。但还是有问题：假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列：

客户端1成功锁住了A, B, C，获取锁成功（但D和E没有锁住）。

节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了。

节点C重启后，客户端2锁住了C, D, E，获取锁成功。

这样，客户端1和客户端2同时获得了锁（针对同一资源）。这个问题与redis的持久化程度有关，redis的AOF持久化默认是每秒持久化1次，因此最多会丢失1秒内的数据，即便是每次操作都持久化，也不能保证没有数据丢失，这与redis的实现无关，是系统决定的。

针对上述节点崩溃的问题，Redlock的作者提出了”延迟重启“的概念，即redis服务器宕机之后，要隔一段时间再重启，这个时间要比锁的过期时间要长，这样当服务器重启之后，所有的锁都过期了，就不存在同时获取锁的问题了。

然而，Redlock还有别的问题，那就是”时钟跳跃“和“长时间GC”带来的锁失效问题。考虑下面的事件时序：

客户端1从Redis节点A, B, C成功获取了锁（多数节点）。由于网络问题，与D和E通信失败。

节点C上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。

客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。

客户端1和客户端2现在都认为自己持有了锁。



上面的问题主要是来自于Redlock的安全性对系统的时钟有较强的依赖，这种依赖性在Redlock的代码可以很直观地体现出来：Redlock使用gettimeofday来判断锁的过期时间，而手册上明确表示，gettimeofday返回的时间会受到系统时间不连续跳跃的影响 ，也就是说，它可能会突然向前跳几分钟，甚至会向后跳。

Redlock的作者对此做了说明：通过适当的运维可以避免时钟发生大的跳跃，同时Redlock也不要求时钟一定是准确无误的，差个几百毫秒是可以接受的。

同样的，长时间的GC也会也会产生类似的问题：客户端1在获取到锁之后进入了“stop the world gc”，且gc时间很长，等客户端1恢复过来的时候，他还是认为自己持有锁，实际上锁已经过期了，客户端1再去处理共享资源就不安全了，因为这时别的客户端也可以获取到锁。

这个问题不是理论上的，实际上他确实会发生，大名鼎鼎的HBase就曾遇到过这个问题。

可能有人会提议说，既然锁可能会过期，那就在处理任务之前再去检查一下锁是否过期不就行了？其实这并不能解决问题，因为gc可能发生在任意时刻，例如在检查锁之后发生。

那么如何看待Redlock呢？

分布式系统专家Martin Kleppmann在一篇文章中提出了一个观点：我们使用分布式锁是出于两种目的——效率（Efficiency）或准确性（Correctness）：

* Efficiency: Taking a lock saves you from unnecessarily doing the same work twice (e.g. some expensive computation). If the lock fails and two nodes end up doing the same piece of work, the result is a minor increase in cost (you end up paying 5 cents more to AWS than you otherwise would have) or a minor inconvenience (e.g. a user ends up getting the same email notification twice).
* Correctness: Taking a lock prevents concurrent processes from stepping on each others’ toes and messing up the state of your system. If the lock fails and two nodes concurrently work on the same piece of data, the result is a corrupted file, data loss, permanent inconsistency, the wrong dose of a drug administered to a patient, or some other serious problem.

大意是：

* 效率：使用分布式来避免相同的任务执行两次，例如昂贵的计算。如果锁失败了，两个节点都会执行相同的任务，造成的损失是很小的（向亚马逊多付5分钱）或者很小的不便利（向用户发送了两封相同的邮件）

* 正确性：使用锁来防止并发进程相互干扰并打乱系统的状态。如果锁失败，两个节点同时处理同一块数据，会导致文件损坏、数据丢失、永久的不一致、给病人用错药物剂量或其他一些严重问题。

基于上述两个目的，Martin Kleppmann形容Redlock “非驴非马”。因为如果是为了效率的话，使用单点的redis做分布式锁就够了，还用不上Redlock；如果是为了准确性的话，也用不上Redlock，因为它不是一个足够安全的算法，它对于系统模型的假设中包含很多危险的成分，尤其是系统时钟。

那么对于准确性要求如何选择分布式锁呢？Martin Kleppmann建议使用zookeeper或者支持事务的数据库。

但是利用zookeeper实现分布式锁也有瑕疵，除了之前所述的效率问题之外，zookeeper的分布式锁也面临着和Redlock类似的问题：长时间的gc会导致锁失效。

原因在于，为了防死锁，我们基于zookeeper实现分布式锁的时候，都是利用临时节点来实现的，这样当客户端断开连接超过一段时间时，zookeeper可以自动删除临时节点，相当于释放掉了锁。但是断开连接有多种可能，可能是客户端宕机了，也可能是遭遇了长时间的gc。zookeeper的作者Flavio Junqueira给出了下面一个例子：

1. 客户端1成功创建了临时节点，获取到了锁

2. 客户端1进入了gc

3. 客户端1对zookeeper服务端无心跳反应，zookeeper删掉了临时节点

4. 客户端2成功创建了临时节点，成功获取到了锁

5. 客户端1从gc恢复，认为自己还是持有锁

6. 客户端1和客户端2同时持有锁，共享资源得不到保护

类似的超时问题也会导致临时节点被删除，所以经常会在网上看到这样的问题：为什么在本地debug之后，zookeeper上的临时节点没了？

因此zookeeper官方文档建议，使用zookeeper作为分布式锁需要注意不要有轮询或者超时（There is no polling or timeouts）。

此外zookeeper官网还给出了一种分布式共享锁（读写锁）的实现方式。大致流程如下：

需要注意的是，当有很多客户端在等待读锁的时候，如果这时写锁被释放了，会引起"羊群效应"，虽然我们常说要避免“羊群效应”，但这种“羊群效应”是合理的，因为所有等待读锁的客户端都应该得到通知被释放。真正意义上的“羊群效应”指的是当只有一个或少量的机器能够继续工作时，释放了大量的机器（“羊群“）。

### 3.4 Cerberus

Cerberus是美团基础架构平台实现了一个集成的分布式锁，他是基于redis、tair和zookeeper这三个引擎实现的，并对外提供了一套api接口，用户可以根据实际场景来选择合适的引擎。

相关链接：mtlocker平台

如何选择引擎呢？

Cerberus推荐使用Tair和Squirrel集群。因为Tair和Squirrel作为缓存，开销和响应时间会比Zk小，QPS比zk高，使用Tair和Squirrel并发量可以达到500/s以上。如果并发量超过1000/s，请尽量避免使用分布式锁。Cerberus性能测试

Tair和Squirrel目前尚未实现读写锁，因此如果业务必须要使用读写锁，那么只能使用zk集群。

目前Tair和Squirrel使用的是非公平锁，Zk使用的是公平锁。如果对公平性有要求的业务，请依此来选择使用。

如果业务方要求非常强的可靠性，那么建议使用zk引擎。

虽说Cerberus支持多引擎，但目前还不支持引擎的切换和降级，因为引擎切换可能会导致锁数据丢失。

正是由于Cerberus集成了多个引擎，所以redis、tair和zookeeper该有的问题Cerberus都有，但这也是他的优势，选择灵活嘛。

要是能提供个管理界面就更nice了。

## 四.分布式锁的其他实现方式

### 4.1 etcd

在讨论Redlock时，Martin Kleppmann提到了分布式算法的一个基础性问题：安全性和活性。即一个好的分布式算法不应做任何系统时钟上的假设，他的安全属性应该是保持不变的，进程暂停、网络延迟、时钟跳跃这些因素只会影响到他的活性，而不会影响到他的安全性，也就是说在具有不安全因素的情况下，算法的性能可能会降低，但不应该给出错误的结果。因此按照这种标准，Martin Kleppmann认为Redlock的安全性是不够的；但是，能够达到这一标准的算法是存在的，比如Paxos和Raft。

etcd就是基于Raft算法的强一致kv存储系统。这意味着某次操作存储到集群中的值必然是全局一致的，这就避免了redis分布式锁的单点问题，所以很容易实现分布式锁来进行资源独占和时序控制：

保持独占。即只有一个用户可以获得锁，etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。

控制时序。即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。

而在Etcd的v3版本官方client里有一个concurrency包，里面就已经实现了分布式锁，基本的原理就是上面那样的。

### 4.2 Chubby

Redlock的一个问题是锁过期，Martin Kleppmann针对该问题提出了一个 fencing token的方案，大概意思是通过一个单调递增的数字来控制过期访问。例子如下：

客户端1先获取到的锁，因此有一个较小的fencing token，等于1，然后进入gc pause了。

客户端1的锁过期了，客户端2获取到锁，有一个较大的fencing token，等于2

客户端1从GC pause中恢复过来之后，依然是向存储服务发送访问请求，但是带了fencing token = 1。存储服务发现它之前已经处理过2的请求，所以会拒绝掉这次1的请求。这样就避免了冲突。

但是，感觉这玩意儿。。。只能说解决了部分问题，如果客户端1从gc恢复过来后，客户端1和客户端2的请求还是按顺序到达服务端的话，而且几乎是同时达到的，那还是有问题，因为还是有两个请求在同时访问共享资源。

当然还有一种方案，既然上述问题是由锁过期导致的，那么可以允许客户端在锁过期之后的一段时间内继续持有锁，这样客户端从gc恢复过来之后，还是继续持有锁，这样就可以避免上述问题，换一种说法就是延长过期锁的释放时间。

其实Chubby就综合了上述两种方案的特点：锁序列和锁延迟，利用这两种机制来优化锁过期的问题：

锁序列：任何时候，锁的持有者都可以向Chubby请求一个锁序列器，其包括锁的名字、锁模式（排他或共享），以及锁序号。当客户端应用程序在进行一些需要锁机制保护的操作时，可以将该锁序列器一并发送给服务端。Chubby服务端接收到这样的请求后，会首先检测该序列器是否有效，以及检查客户端是否处于恰当的锁模式；如果没有通过检查，那么服务端就会拒绝该客户端请求。 有点类似Martin Kleppmann提到的fencing token的方案。

锁延迟：如果一个客户端以正常的方式主动释放了一个锁，那么Chubby服务端将会允许其他客户端能够立即获取到该锁。而如果一个锁是因为客户端的异常情况（如客户端无响应）而被释放的话，那么Chubby服务器会为该锁保留一定的时间，我们称之为“锁延迟”（lock-delay）。

好了，没啦！！！

参考资料

https://redis.io/topics/distlock

https://blog.csdn.net/kongmin_123/article/details/82081953

https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html

https://fpj.me/2016/02/10/note-on-fencing-and-distributed-locks/

https://www.infoq.cn/article/etcd-interpretation-application-scenario-implement-principle

https://blog.csdn.net/qq_21125183/article/details/86479700
